---
layout: post
title: "Day 16 Code Implementation"
date: 2025-06-17
author: Christian Olabisi
permalink: /day16.html
tags: ["Code implementation"]

what_i_learned: |
  We are using Resnet-18 as an architecture for our algorithm. Resnet-18 introduces skip connections that allow for gradient flow. ResNet-18 has 18 layers, making it deep enough to learn complex features. Also, Resnet-18 does well with classification tasks. 
The ResNet-18 is important for our research because it is commonly used. I also learned more about Cross-entropy loss. Cross-entropy loss  is the standard loss function for multi-class classification. For a single example, it's calculated as Loss =                     - log(p_correct_class). Were using cross entropy because our outputs can be interpreted as class probabilities after something called softmax. Which is when you convert real numbers into probabilities. It also is a great tool when handling Cifar-10. Lastly, it provides strong gradients when predictions are wrong. Meaning it's weaker when the predictions are correct. Looked a little bit at the optimizer configuration. I plan on taking a deeper look at how the optimizer configuration works tomorrow.
  
blockers: |
  No blockers

reflection: |
  My code is almost done running, Will look at the results to see if my accuracy has improved. Also, I will be looking more at the breakdown of some other components of my code.
---

