---
layout: post
title: "Day 15"
date: 2025-06-16
author: Christian Olabisi
permalink: /day15.html
tags: ["Code implementation",]

what_i_learned: |
  I learned that applying stronger data augmentation and extending the training schedule with the appropriate learning rate scheduler, significantly improves the modelâ€™s performance. I also learned that by simulating  adversarial attacks like the PGD and then applying a defense before the final inference, you can assess and enhance the robustness of the  model. Lastly based on the code I was implementing today I learned that incorporating metrics like training accuracy alongside loss ensures that the code not only optimizes the model but also tracks how well it learns to recognize patterns over time. This will provide early warnings for issues like overfitting or underfitting and guide potential adjustments needed for the training loop.
  
blockers: |
  No blockers

reflection: |
  The learning rate scheduler wasn't needed to run my code. It gave me a lot of issues when I would run my final code. I tried to fix this issue by adding and downloading the needed library files, but each time I did and tried to run the code it would fail. So I ended up removing the learning scheduler and just importing some torch files. I finally got my code to run and now I'm just waiting for the results which come in by tomorrow morning. Hopefully my accuracy should increase from the last time  I ran this algorithm
---
